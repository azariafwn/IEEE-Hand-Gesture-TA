\section{Related Work}

Hand gesture recognition for home automation has been widely explored using various approaches, ranging from computer vision to deep learning. In the context of edge-based implementations, Kurian et al. \cite{kurian2023visual} developed a vision-based home automation system using MediaPipe on a Raspberry Pi. Their system successfully controlled appliances using static poses, such as American Sign Language (ASL) alphabets 'A' and 'B', achieving 95\% accuracy. However, their approach was limited to static gestures, which lack the intuitiveness and complexity required for more natural interaction compared to dynamic movements.

To address the limitations of static recognition, Deep Learning models capable of handling temporal sequences, such as Long Short-Term Memory (LSTM), have been adopted. Gurrala et al. \cite{gurrala2025lstm} demonstrated the effectiveness of LSTM for dynamic gesture recognition in a sign language translation system, achieving an impressive accuracy of 99.6\%. While this study validated the superior performance of LSTM for sequential data, it did not address the challenges of deploying such computationally intensive models on resource-constrained edge devices.

Recently, Lopes et al. \cite{lopes2026geco} introduced GECO, a real-time computer vision-assisted gesture controller for IoT smart homes. Deployed on Android mobile devices, the system utilizes MediaPipe to enable contactless control and analog adjustments with ultra-low latency. However, its reliance on a mobile application paradigm introduces significant practical friction. To interact with the system, users must physically engage with their smartphone to unlock the device and launch the application before performing gestures. This prerequisite arguably diminishes the core advantage of contactless interaction, as traditional touch controls are often more efficient once the device is already in hand. 

To achieve true "ambient intelligence" in a smart home, the recognition system requires an "always-on" sensing node, like a surveillance camera, that remains perpetually on standby for inference without explicit user initiation. This paper bridges this gap by implementing an optimized, quantized LSTM model specifically designed to run continuously on a standalone Raspberry Pi 5 edge device, providing a truly contactless and ambient dynamic gesture control solution.